[model]
thread_num = 64
pretrain_iter = 1
train_iter = 100
word_num = 10
letter_num = 10
observation_dim = 5
trunc = None

[pyhlm]
num_states = ${model:word_num}
alpha = 10
gamma = 10
init_state_concentration = 10

[letter_observation]
mu_0 = numpy.zeros(${model:observation_dim})
sigma_0 = numpy.identity(${model:observation_dim})
kappa_0 = 0.01
nu_0 = ${model:observation_dim} + 5

[letter_duration]
alpha_0 = 200
beta_0 = 10

[letter_hsmm]
alpha = 10
gamma = 10
init_state_concentration = 10

[word_length]
alpha_0 = 30
beta_0 = 10000
